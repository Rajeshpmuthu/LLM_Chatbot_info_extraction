{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "10CDnBX54dhqdok4nvQtKiZSesRoWCCOM",
      "authorship_tag": "ABX9TyMCo8EesKDg/vyJKWeHtJpQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajeshpmuthu/LLM_Chatbot_info_extraction/blob/main/Chatbot_Info_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbOi4XdTurzq",
        "outputId": "0a1dff71-c230-425e-d38e-bf538aae43ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import math\n",
        "import time\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "from langchain.tools import BaseTool\n",
        "from typing import Optional, Type\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Type, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "import openai\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain.chains import create_extraction_chain\n",
        "\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import MessagesPlaceholder\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.tools import BaseTool\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.schema import SystemMessage"
      ],
      "metadata": {
        "id": "bPJ6HoTGuzvi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"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\""
      ],
      "metadata": {
        "id": "xqjPQujVu6C8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVvsLUN6vBDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "**ChatBot Class:** Defines a ChatBot class with methods for initializing, continuing a conversation, extracting user information, and printing messages.\n",
        "\n",
        "**Initialization:** Initializes the ChatBot with an OpenAI API key, default model configuration, and an empty list to store messages.\n",
        "\n",
        "**Adding Initial Messages:** Adds initial system and AI messages to the conversation to establish a friendly and persuasive tone.\n",
        "\n",
        "**Message Handling:** Provides methods to add messages to the conversation and continue the conversation with user input.\n",
        "\n",
        "**User Information Extraction:** Uses langchain module create_extraction_chain defines a schema for extracting user information such as name, email, phone number, address, date of birth, and education.\n",
        "\n",
        "**Conversation Closure:** Checks if the conversation is closed based on the response and extracts user information when appropriate.\n",
        "\n",
        "**Printing Messages:** Provides a method to print all messages in the conversation.\n",
        "\n",
        "**Main Execution:** In the main section of the code, it initializes the ChatBot and sets up an interactive chat interface.\n",
        "\n",
        "**Interactive Chat Interface:** Creates an interface for users to input messages and receive responses from the chatbot. It also handles the closure of the conversation and extracts user information.\n",
        "\n",
        "**CSV Data Storage:** Stores the extracted user information in a CSV file named 'user_info.csv' and prints the extracted data.\n",
        "\n",
        "**Graphical User Interface (GUI):** Utilizes a graphical library (possibly gr) to create a chatbot interface with text input and message display areas.\n",
        "\n",
        "Launching GUI: Launches the GUI to enable users to interact with the chatbot."
      ],
      "metadata": {
        "id": "TV2lPSCevBrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatBot:\n",
        "    def __init__(self, openai_api_key, model='gpt-3.5-turbo'):\n",
        "        # Initialize the ChatBot with OpenAI API key and model configuration\n",
        "        self.chat = ChatOpenAI(\n",
        "            openai_api_key=openai_api_key,\n",
        "            temperature=0.3,\n",
        "            model=model,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        self.messages = []  # Initialize an empty list to store messages\n",
        "\n",
        "        # Add initial system and AI messages to the conversation\n",
        "        self.add_message(SystemMessage(content=(\"\"\"You are a friendly persuader your goal is to create an engaging and persuasive conversational experience for users to willingly share their details.\n",
        "                                                    The users may not answer correctly, may ask questions before deciding to answer, refuse to answer until convinced about the reason.\n",
        "                                                    Hence your role is that of a friendly persuader, seeking to validate and gather information in a natural conversation. You have to convince to give the following details:\n",
        "                                                    Name, email, phone no, Address, Date of birth, Education.\n",
        "\n",
        "                                                    Things to note about conversational flow:\n",
        "                                                    1. Don't be too pushy and formal, be friendly, interactive, and trustworthy\n",
        "                                                    2. Convince a little bit, if still the user doesn't want's to give the detail, move on to the next detail\n",
        "                                                    3. You don't have to be overpushy proving yourself trustworthy everytime but rather be friendly and convincing\n",
        "                                                    4. If the user declines to give a detail ask another detail but don't end conversation until you have asked all details\n",
        "\n",
        "                                                    (Note: These are some things that you should strictly follow:\n",
        "                                                    1. Design a coherent conversation flow where users will easily give the information or convince well to give their info.\n",
        "                                                    2. Details to extract: Name, email, phone no, Address, Date of birth, Education.\n",
        "                                                    3. Make sure the Chat flow is consistent or natural, and minimum hallucinations (question repetitions, out-of-context questions) shouldn't happen.\n",
        "                                                    4. You have to Verify user information\n",
        "                                                    5. If you can't get all the fields it's fine just save whatever you can get from the user. No matter what you have to do to persuade the user to give the information, ask him questions if he's hesitant and then again persuade him in more friendly and trustworthy way but if he's not willing to give after lot of convincing more forward with with the next detail\n",
        "                                                    6. After all the detials are covered doesn't matter if some of them are not provided compulsary respond: \"Thank you for providing all the details\" exactly same wording nothing else\n",
        "                                                    7. Again! you have to end with the response \"Thank you for providing all the details\" EXACT SAME SENTENCE )\"\"\"\n",
        "                                                            )))\n",
        "        self.add_message(AIMessage(content=\"\"\"Hello! I'm your friendly AI companion, here to create an enjoyable and meaningful conversation. Our goal is to make your experience engaging, informative, and fun. Is it alright if we have a chat today?\n",
        "                                                      \"\"\"))\n",
        "\n",
        "    def add_message(self, message):\n",
        "        # Add a message to the conversation\n",
        "        self.messages.append(message)\n",
        "\n",
        "    def continue_conversation(self, user_input):\n",
        "        # Continue the conversation with user input\n",
        "        human_message = HumanMessage(content=user_input)\n",
        "        self.add_message(human_message)\n",
        "        response = self.chat(self.messages)\n",
        "        self.add_message(response)\n",
        "        return response\n",
        "\n",
        "    def extract_user_info(self):\n",
        "        # Combine user messages for extraction\n",
        "        user_input = \" \".join([message.content for message in self.messages if isinstance(message, HumanMessage)])\n",
        "\n",
        "        # Define your extraction schema\n",
        "        schema = {\n",
        "            \"properties\": {\n",
        "                \"Name\": {\"type\": \"string\"},\n",
        "                \"email\": {\"type\": \"string\"},\n",
        "                \"phone no\": {\"type\": \"integer\"},\n",
        "                \"Address\": {\"type\": \"string\"},\n",
        "                \"Date of birth\": {\"type\": \"string\"},\n",
        "                \"Education\": {\"type\": \"string\"},\n",
        "            },\n",
        "            \"required\": [],\n",
        "        }\n",
        "\n",
        "        # Run the extraction chain\n",
        "        llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0, model=\"gpt-3.5-turbo\")\n",
        "        chain = create_extraction_chain(schema, llm)\n",
        "        extracted_info = chain.run(user_input)\n",
        "        return extracted_info\n",
        "\n",
        "    def print_messages(self):\n",
        "        # Print all messages in the conversation\n",
        "        for message in self.messages:\n",
        "            print(message.content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot = ChatBot(openai_api_key=OPENAI_API_KEY)\n",
        "    conversation_closed = False\n",
        "\n",
        "    def add_text(history, text):\n",
        "        global prompt\n",
        "        prompt = text\n",
        "        history = history + [(text, None)]\n",
        "\n",
        "        return history, gr.update(value=\"\", interactive=False)\n",
        "\n",
        "    def chatbot_response(history):\n",
        "        global conversation_closed\n",
        "        response = chatbot.continue_conversation(prompt).content\n",
        "        # Check if the conversation is closed\n",
        "        if \"all the details\" in response or \"all the information\"  in response:\n",
        "            conversation_closed = True\n",
        "\n",
        "            if conversation_closed:\n",
        "                # Extract user information and print it\n",
        "                extracted_info = chatbot.extract_user_info()\n",
        "                print(\"Extracted User Information:\")\n",
        "                print(extracted_info)\n",
        "\n",
        "                import pandas as pd\n",
        "                # Define the CSV file path\n",
        "                csv_file_path = 'user_info.csv'\n",
        "\n",
        "                # Create or open the CSV file\n",
        "                try:\n",
        "                    df = pd.read_csv(csv_file_path)\n",
        "                except FileNotFoundError:\n",
        "                    df = pd.DataFrame()\n",
        "\n",
        "                # Append the extracted information to the DataFrame\n",
        "                df = df.append(extracted_info, ignore_index=True)\n",
        "\n",
        "                # Save the updated DataFrame to the CSV file\n",
        "                df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "                response += \"\\nHere's the data extracted:\\n\" + str(extracted_info)\n",
        "\n",
        "        history[-1][1] = \"\"\n",
        "        for character in response:\n",
        "            history[-1][1] += character\n",
        "            time.sleep(0.005)\n",
        "            yield history\n",
        "\n",
        "    height = 800\n",
        "    width_0, width_1 = 400, 800\n",
        "    scale_0, scale_1 = width_0 // math.gcd(width_0, width_1), width_1 // math.gcd(width_0, width_1)\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                chatbot_interface = gr.Chatbot([(None, \"\"\"Hello! ğŸ˜„ I'm here to have an enjoyable and meaningful conversation with you.\n",
        "                Our goal is to make your experience engaging, informative, and fun. Would you like to chat with me today? \"\"\")], elem_id=\"chatbot\", bubble_full_width=False)\n",
        "\n",
        "                with gr.Row():\n",
        "                    txt = gr.Textbox(show_label=False,\n",
        "                                      placeholder=\"Type your message here...\"\n",
        "                                      )\n",
        "\n",
        "                    txt_msg = txt.submit(add_text, [chatbot_interface, txt], [chatbot_interface, txt], queue=False).then(\n",
        "                        chatbot_response, chatbot_interface, chatbot_interface)\n",
        "\n",
        "                    txt_msg.then(lambda: gr.update(interactive=True), None, [txt], queue=False)\n",
        "\n",
        "    demo.queue()\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "UDPeoiMAvDsM",
        "outputId": "2756c373-7a93-4dd9-8671-480b3587ad58"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://8198ee3a28879204cf.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8198ee3a28879204cf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5kFyKhBvJJ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}